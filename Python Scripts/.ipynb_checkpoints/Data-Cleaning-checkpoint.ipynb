{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "956\n",
      "(742, 15)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Dataset/Data_Cleaned.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9f8802c133e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[0mdf_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unnamed: 0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m \u001b[0mdf_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./Dataset/Data_Cleaned.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   3202\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3203\u001b[0m         )\n\u001b[1;32m-> 3204\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3206\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m             f, handles = get_handle(\n\u001b[0m\u001b[0;32m    185\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Dataset/Data_Cleaned.csv'"
     ]
    }
   ],
   "source": [
    "#import pandas to create DataFrames as pd\n",
    "import pandas as pd \n",
    "\n",
    "#Importing our Dataset into a DataFrame for Cleaning\n",
    "df = pd.read_csv('../Dataset/glassdoor_jobs.csv')\n",
    "\n",
    "#Displaying first 5 result to see if our dataset is loaded correctly\n",
    "df.head()\n",
    "\n",
    "#seeing how many rows and columns we have in our dataset\n",
    "df.shape\n",
    "\n",
    "\n",
    "## TASK 1: PARSING SALARIES\n",
    "\n",
    "#Looking what kind of enteries our 'Salary Estimate' columns has\n",
    "x = len(df['Salary Estimate'])\n",
    "\n",
    "print(x)\n",
    "#we see that there are multiple enteries such as -1, per hour and \n",
    "#removing rows containing '-1' as entries in 'Salary Estimate Column by putting a condition'\n",
    "df = df[df['Salary Estimate'] != \"-1\"]\n",
    "\n",
    "#seeing how many rows we have eliminated containing -1 values\n",
    "print(df.shape)\n",
    "\n",
    "#removing keyword '(Glassdoor est.) from our 'Salary Estimate' Colummn\n",
    "salary = df['Salary Estimate'].apply(lambda x: x.split('(')[0])\n",
    "\n",
    "#removing $ and K sign from our Salary Estimate\n",
    "minus_kd = salary.apply(lambda x: x.replace('K','').replace('$',''))\n",
    "\n",
    "\n",
    "#Creating a new column indicating if the data entry in Salary Estimate Columns is per hour or not\n",
    "df['hourly'] = df['Salary Estimate'].apply(lambda x: 1 if 'per hour' in x.lower() else 0) \n",
    "\n",
    "#Creating a new column indicating if the data entry in Salary Estimate Columns is employer provided or not\n",
    "df['employer provided'] = df['Salary Estimate'].apply(lambda x: 1 if 'employer' in x.lower() else 0) \n",
    "\n",
    "min_hr = minus_kd.apply(lambda x: x.lower().replace('per hour','').replace('employer provided salary:',''))\n",
    "#removing per hour and employer provided salary: keyword from our Dataset\n",
    "df['min_salary'] = min_hr.apply(lambda x: x.split('-')[0]).astype('int32')\n",
    "df['max_salary'] = min_hr.apply(lambda x: x.split('-')[1]).astype('int32')\n",
    "\n",
    "df['avg_salary'] = (df.min_salary+df.max_salary)/2\n",
    "df.head()\n",
    "\n",
    "# TASK 2: LOCATION PARSING\n",
    "df['job_state'] = df['Location'].apply(lambda x: x.split(',')[1])\n",
    "\n",
    "#seeing if the job is in company's headquarter\n",
    "df['same_state'] = df.apply(lambda x: 1 if x.Location == x.Headquarters else 0, axis=1)\n",
    "df.head()\n",
    "\n",
    "#TASK 3: AGE OF COMPANY\n",
    "df['age'] = df.Founded.apply(lambda x: x if x < 1 else 2021 - x )\n",
    "df.head()\n",
    "\n",
    "\n",
    "#TASK 4:  PARSING JOB DECSCRIPTION\n",
    "#Seeing if the python programming langauge is required in job description\n",
    "df['python'] = df['Job Description'].apply(lambda x: 1 if 'python' in x.lower() or 'python,' in x.lower() else 0)\n",
    "df['python'].value_counts()\n",
    "\n",
    "#Seeing if the R programming langauge is required in job description\n",
    "df['R'] = df['Job Description'].apply(lambda x: 1 if 'r-studio'in x.lower() or 'r studio' in x.lower() or ' r ' in x.lower() or ' r,' in x.lower() else 0)\n",
    "df['R'].value_counts()\n",
    "\n",
    "#Seeing if the Spark is required in job description\n",
    "df['spark'] = df['Job Description'].apply(lambda x: 1 if 'spark' in x.lower() else 0)\n",
    "df['spark'].value_counts()\n",
    "\n",
    "#Seeing if the AWS is required in job description\n",
    "df['aws'] = df['Job Description'].apply(lambda x: 1 if 'aws' in x.lower() else 0)\n",
    "df['aws'].value_counts()\n",
    "\n",
    "\n",
    "#Seeing if the Excel is required in job description\n",
    "df['excel'] = df['Job Description'].apply(lambda x: 1 if 'excel' in x.lower() or 'ms excel' in x.lower() or 'microsoft excel' in x.lower() else 0)\n",
    "df['excel'].value_counts()\n",
    "\n",
    "df.head()\n",
    "\n",
    "df_out = df.drop('Unnamed: 0', axis = 1)\n",
    "df_out.to_csv('../Dataset/Data_Cleaned.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
